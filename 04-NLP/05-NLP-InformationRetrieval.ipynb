{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-info\"> 1. Introduction </div>\n",
    "\n",
    "1.  **Problem**:  Processing raw text intelligently is difficult: most words are rare, and it's common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. It's usually better to use linguistic knoweledge to add useful information.\n",
    "\n",
    "\n",
    "2. **Part of Speed Tagging**: It refers to the way words are arranged together as a single unit of phrase.\n",
    "\n",
    "+ Coarse-grained POS tags\n",
    "\n",
    "+ Fine-grained POS tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-info\"> 2. Setup </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install networkx\n",
    "#!pip install datapane\n",
    "#!pip install operator\n",
    "#!pip install pyvis\n",
    "#!pip install streamlit pyvis networkx\n",
    "#!pip install langdetect\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import zipfile\n",
    "import json\n",
    "import urllib\n",
    "import langdetect\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy \n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community #This part of networkx, for community detection, needs to be imported separately\n",
    "import datapane as dp\n",
    "#from operator import itemgetter\n",
    "from pyvis.network import Network\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div class=\"alert alert-info\"> 3. Data Preparation </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-YAVgrS6CUM"
   },
   "source": [
    " ## <font color=red>**3.1.  News category dataset**\n",
    "\n",
    "The dataset used is “News category dataset” from Kaggle (https://www.kaggle.com/rmisra/news-category-dataset. This dataset provided with around 200k news headlines from the year 2012 to 2018 obtained from HuffPost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>3.2. Load data\n",
    "    \n",
    "The dataset is contained into a json file, so I will first read it into a list of dictionaries with json and then transform it into a pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26768, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>U.S. Launches Auto Import Probe, China Vows To...</td>\n",
       "      <td>The investigation could lead to new U.S. tarif...</td>\n",
       "      <td>U.S. Launches Auto Import Probe, China Vows To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Starbucks Says Anyone Can Now Sit In Its Cafes...</td>\n",
       "      <td>The new policy was unveiled weeks after the co...</td>\n",
       "      <td>Starbucks Says Anyone Can Now Sit In Its Cafes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Seattle Passes Controversial New Tax On City's...</td>\n",
       "      <td>Following the council vote, Amazon’s vice pres...</td>\n",
       "      <td>Seattle Passes Controversial New Tax On City's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Uber Ends Forced Arbitration In Individual Cas...</td>\n",
       "      <td>Victims will be free to go to court -- but a f...</td>\n",
       "      <td>Uber Ends Forced Arbitration In Individual Cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Chili's Hit By Data Breach, Credit And Debit C...</td>\n",
       "      <td>The breach is believed to have occurred betwee...</td>\n",
       "      <td>Chili's Hit By Data Breach, Credit And Debit C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           headline  \\\n",
       "0  BUSINESS  U.S. Launches Auto Import Probe, China Vows To...   \n",
       "1  BUSINESS  Starbucks Says Anyone Can Now Sit In Its Cafes...   \n",
       "2  BUSINESS  Seattle Passes Controversial New Tax On City's...   \n",
       "3  BUSINESS  Uber Ends Forced Arbitration In Individual Cas...   \n",
       "4  BUSINESS  Chili's Hit By Data Breach, Credit And Debit C...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  The investigation could lead to new U.S. tarif...   \n",
       "1  The new policy was unveiled weeks after the co...   \n",
       "2  Following the council vote, Amazon’s vice pres...   \n",
       "3  Victims will be free to go to court -- but a f...   \n",
       "4  The breach is believed to have occurred betwee...   \n",
       "\n",
       "                                                text  \n",
       "0  U.S. Launches Auto Import Probe, China Vows To...  \n",
       "1  Starbucks Says Anyone Can Now Sit In Its Cafes...  \n",
       "2  Seattle Passes Controversial New Tax On City's...  \n",
       "3  Uber Ends Forced Arbitration In Individual Cas...  \n",
       "4  Chili's Hit By Data Breach, Credit And Debit C...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/News_Category.csv')\n",
    "print(df.shape)\n",
    "## print 5 random rows\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KIR46g5Y2uQs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRAVEL          9855\n",
       "FOOD & DRINK    6208\n",
       "BUSINESS        5878\n",
       "SPORTS          4827\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['category'] == 'TRAVEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>**3.3. Text Processing and Normalizing** </font>  \n",
    "    \n",
    "Before feature engineering, we need to  pre-process, clean, and normalize text. Following are some of the popular pre-processing techniques:\n",
    "\n",
    "**1. Text tokenization and lower casting**: Split doc into individual words and lower casting all words <br>\n",
    "**2. Removing special characters**: remove special characters and punctuations <br>\n",
    "**3. Removing stop words**: Words like \"a\" and \"the\" appear so frequently and  are called stop words, they can be filtered from the text to be processed <br>\n",
    "**4. Stemming**: extract root of words by remove -ing, -ly, -ed... <br>\n",
    "**5. Lemmatization**: In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess a string.\n",
    ":parameter\n",
    "    :param text: string - name of column containing text\n",
    "    :param lst_stopwords: list - list of stopwords to remove\n",
    "    :param flg_stemm: bool - whether stemming is to be applied\n",
    "    :param flg_lemm: bool - whether lemmitisation is to be applied\n",
    ":return\n",
    "    cleaned text\n",
    "'''\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=False, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and   characters and then strip)\n",
    "    #text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    #text = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    text = str(text).lower().strip()\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>14 Ways To Make Family Road Trips Easier, From...</td>\n",
       "      <td>Having waterproof covers on the seats is kind ...</td>\n",
       "      <td>14 Ways To Make Family Road Trips Easier, From...</td>\n",
       "      <td>having waterproof covers on the seats is kind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>14 Trips To Take From New York City On A Long ...</td>\n",
       "      <td>Charming towns, relaxing beaches and top hikin...</td>\n",
       "      <td>14 Trips To Take From New York City On A Long ...</td>\n",
       "      <td>charming towns, relaxing beaches and top hikin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>Disney Reveals Opening Seasons For 'Star Wars'...</td>\n",
       "      <td>Star Wars: Galaxy's Edge will open at Disneyla...</td>\n",
       "      <td>Disney Reveals Opening Seasons For 'Star Wars'...</td>\n",
       "      <td>star wars: galaxy's edge will open at disneyla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>Lonely Planet's Top European Destinations Of 2...</td>\n",
       "      <td>These underrated travel destinations in Europe...</td>\n",
       "      <td>Lonely Planet's Top European Destinations Of 2...</td>\n",
       "      <td>these underrated travel destinations in europe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>8 Majestic Islands In Europe That Most Tourist...</td>\n",
       "      <td>If you’re dreaming about a romantic European g...</td>\n",
       "      <td>8 Majestic Islands In Europe That Most Tourist...</td>\n",
       "      <td>if you’re dreaming about a romantic european g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                                           headline  \\\n",
       "5878   TRAVEL  14 Ways To Make Family Road Trips Easier, From...   \n",
       "5879   TRAVEL  14 Trips To Take From New York City On A Long ...   \n",
       "5880   TRAVEL  Disney Reveals Opening Seasons For 'Star Wars'...   \n",
       "5881   TRAVEL  Lonely Planet's Top European Destinations Of 2...   \n",
       "5882   TRAVEL  8 Majestic Islands In Europe That Most Tourist...   \n",
       "\n",
       "                                      short_description  \\\n",
       "5878  Having waterproof covers on the seats is kind ...   \n",
       "5879  Charming towns, relaxing beaches and top hikin...   \n",
       "5880  Star Wars: Galaxy's Edge will open at Disneyla...   \n",
       "5881  These underrated travel destinations in Europe...   \n",
       "5882  If you’re dreaming about a romantic European g...   \n",
       "\n",
       "                                                   text  \\\n",
       "5878  14 Ways To Make Family Road Trips Easier, From...   \n",
       "5879  14 Trips To Take From New York City On A Long ...   \n",
       "5880  Disney Reveals Opening Seasons For 'Star Wars'...   \n",
       "5881  Lonely Planet's Top European Destinations Of 2...   \n",
       "5882  8 Majestic Islands In Europe That Most Tourist...   \n",
       "\n",
       "                                             text_clean  \n",
       "5878  having waterproof covers on the seats is kind ...  \n",
       "5879  charming towns, relaxing beaches and top hikin...  \n",
       "5880  star wars: galaxy's edge will open at disneyla...  \n",
       "5881  these underrated travel destinations in europe...  \n",
       "5882  if you’re dreaming about a romantic european g...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "df['text_clean'] = df[\"short_description\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=False, lst_stopwords=None))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>3.4.  Split text to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16747, 1)\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "sent_list = []\n",
    "id_list = []\n",
    "category_list = []\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    doc = nlp(df.text_clean.iloc[i])\n",
    "    for sent in doc.sents:\n",
    "        id_list.append(i)\n",
    "        sent = \"'\" + str(sent) + \"'\"\n",
    "        sent_list.append(sent)\n",
    "        category_list.append(df.category.iloc[i])\n",
    "        \n",
    "sent_df = pd.DataFrame()\n",
    "#sent_df['id'] = id_list\n",
    "sent_df['text_clean'] = sent_list\n",
    "print(sent_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16747, 1)\n"
     ]
    }
   ],
   "source": [
    "#sent_df = pd.read_csv('Category_News_sent_df.csv')\n",
    "sent_df = pd.DataFrame()\n",
    "#sent_df['id'] = id_list\n",
    "sent_df['text_clean'] = sent_list\n",
    "print(sent_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'having waterproof covers on the seats is kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'charming towns, relaxing beaches and top hiki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'star wars: galaxy's edge will open at disneyl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'these underrated travel destinations in europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'if you’re dreaming about a romantic european ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean\n",
       "0  'having waterproof covers on the seats is kind...\n",
       "1  'charming towns, relaxing beaches and top hiki...\n",
       "2  'star wars: galaxy's edge will open at disneyl...\n",
       "3  'these underrated travel destinations in europ...\n",
       "4  'if you’re dreaming about a romantic european ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div class=\"alert alert-info\"> 4. LINGUISTICS </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>4.1. Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN      COARSE   FINE   DESCRIPTION FINE                                   DEPENDENCY DESCRIPTION DEPENDENCY\n",
      "-----      ------   ----   ----------------                                   ---------- ----------------------\n",
      "according  VERB     VBG    verb, gerund or present participle                 prep     prepositional modifier\n",
      "to         ADP      IN     conjunction, subordinating or preposition          prep     prepositional modifier\n",
      "a          DET      DT     determiner                                         det      determiner\n",
      "newly      ADV      RB     adverb                                             advmod   adverbial modifier\n",
      "released   VERB     VBN    verb, past participle                              amod     adjectival modifier\n",
      "report     NOUN     NN     noun, singular or mass                             pobj     object of preposition\n",
      ",          PUNCT    ,      punctuation mark, comma                            punct    punctuation\n",
      "the        DET      DT     determiner                                         det      determiner\n",
      "united     PROPN    NNP    noun, proper singular                              compound compound\n",
      "states     PROPN    NNP    noun, proper singular                              nsubjpass nominal subject (passive)\n",
      "is         AUX      VBZ    verb, 3rd person singular present                  auxpass  auxiliary (passive)\n",
      "predicted  VERB     VBN    verb, past participle                              ROOT     None\n",
      "to         PART     TO     infinitival \"to\"                                   aux      auxiliary\n",
      "win        VERB     VB     verb, base form                                    xcomp    open clausal complement\n",
      "the        DET      DT     determiner                                         det      determiner\n",
      "most       ADJ      JJS    adjective, superlative                             amod     adjectival modifier\n",
      "medals     NOUN     NNS    noun, plural                                       dobj     direct object\n",
      "at         ADP      IN     conjunction, subordinating or preposition          prep     prepositional modifier\n",
      "the        DET      DT     determiner                                         det      determiner\n",
      "london     PROPN    NNP    noun, proper singular                              nmod     modifier of nominal\n",
      "2012       NUM      CD     cardinal number                                    nummod   numeric modifier\n",
      "olympics   NOUN     NNS    noun, plural                                       pobj     object of preposition\n",
      ".          PUNCT    .      punctuation mark, sentence closer                  punct    punctuation\n"
     ]
    }
   ],
   "source": [
    "sentence = 'singapore is currently the most competitive city in the world, beating out new york and london, according to the economist'\n",
    "sentence = 'challengers to silicon valley include new york, l.a., boston, tel aviv, and london.'\n",
    "sentence = 'ed young is the senior pastor of fellowship church, which is headquartered in grapevine, texas but has rapidly expanded across texas, to florida, london (uk) and online.'\n",
    "sentence = 'she earned her bfa at london college of communication, uk'\n",
    "sentence = 'london -- a classic mercedes-benz race car driven by formula 1 legend juan manuel fangio sold for 19.6 million pounds ($29.6'\n",
    "sentence  = 'prices soared as people scrambled to flee the london bridge attack'\n",
    "sentence= \"at a london branch of britain's biggest retailer, tesco , which found horse dna in some of its own-brand frozen spaghetti\"\n",
    "sentence = 'maria perez is the co-founder and product manager of glassful'\n",
    "sentence = 'london is a tourist’s paradise owing to the several iconic attractions it has to offer.therefore,it makes sense why london'\n",
    "sentence = \"investigators in washington and london last month struck a $450 million settlement with barclays in a rate-rigging case, but\"\n",
    "sentence = 'according to a newly released report, the united states is predicted to win the most medals at the london 2012 olympics.'\n",
    "\n",
    "# Make your Doc object and pass it into the scorer:\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# For practice, visualize your fine-grained POS tags (shown in the third column):\n",
    "print(f\"{'TOKEN':{10}} {'COARSE':{8}} {'FINE':{6}} {'DESCRIPTION FINE':{50}} {'DEPENDENCY':{6}} {'DESCRIPTION DEPENDENCY'}\")\n",
    "print(f\"{'-----':{10}} {'------':{8}} {'----':{6}} {'----------------':{50}} {'----------':{6}} {'----------------------'}\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_):{50}} {token.dep_:{8}} {spacy.explain(token.dep_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>4.2. Name Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div class=\"alert alert-info\"> 5. Information Retrieval </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>5.1. Find documents contains words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tdf = df[df['text_clean'].str.contains(\"vietnam\")]\n",
    "print(tdf.shape)\n",
    "tdf = tdf.reset_index()\n",
    "\n",
    "\n",
    "for i in range(0,18):\n",
    "    print(\"\\n\", tdf.text.iloc[i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'the variety of places to see and visit, the comfortable weather, the great street food and the sense of tourism all make northern vietnam an easy place to travel.'\n",
    "'from white sand beaches and turquoise water to black rock forests and mountains that hug the clouds, here are some of the most amazing landscapes we came across when we visited vietnam.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 1)\n",
      "\n",
      " 'i have had a love hate relationship with singapore ever since i first visited 15 years ago.'\n",
      "\n",
      " 'singapore airlines' suites class product, available on the airbus a380, made headlines in 2014 after a blogger published his review of the lavish, first-class experience.'\n",
      "\n",
      " 'and besides being cheap, the region offers some of the world's best party countries, like thailand and singapore.'\n",
      "\n",
      " 'the raffles hotel group is spreading its wings from its singapore base.'\n",
      "\n",
      " 'new york city doesn't lack for good food, but i've yet to find satisfying singapore hawker (street food) fare.'\n",
      "\n",
      " 'though singapore air's 18-hour nonstop flight from newark to singapore was suspended last year, there are still plenty of'\n",
      "\n",
      " 'the bridge gives you killer views of singapore city and its islands both day and night.'\n",
      "\n",
      " 'scoot (singapore) to “scoot” implies a jerking, tugging motion that just makes us uncomfortable to think of while soaring'\n",
      "\n",
      " 'he spent two decades life managing marketing for singapore airlines, all nippon airways, and hilton.'\n",
      "\n",
      " 'before moving to singapore last year, i was an avowed anti-tea-ite.'\n",
      "\n",
      " 'shiok, if you aren't familiar with this part of the equator, is a local slang in singapore that describes all feelings that's fantastic, indescribable or sensational'\n",
      "\n",
      " 'since moving to singapore seven months ago, i have become blasé about securing my bike to anything sturdy by means of using something sturdy.'\n",
      "\n",
      " 'singapore is unique amongst foreign destinations.'\n",
      "\n",
      " 'most people would equate the name raffles with the iconic singaporean hotel, and, by association, the famous singapore sling'\n",
      "\n",
      " 'it has been described as a mix of selfishness, pushiness, and competitiveness, plus a fear of losing or missing out, and is said to be characteristic of the people of singapore and malaysia.'\n",
      "\n",
      " 'the catch 22 of the singapore food scene is that there is no culture of institutional continuity in this lucrative and well-loved food industry.'\n",
      "\n",
      " 'since i moved to singapore three months ago, my travels have taken me to malaysia, thailand and the philippines.'\n",
      "\n",
      " 'from designer shopping in london to personal stylist services in singapore, these hotels are ideally situated to offer fashionista's the best shopping these cities have to offer.'\n"
     ]
    }
   ],
   "source": [
    "sent_df.text_clean = sent_df.text_clean.astype(str) \n",
    "\n",
    "\n",
    "sdf = sent_df[sent_df['text_clean'].str.contains(\"singapore\")]\n",
    "print(sdf.shape)\n",
    "sdf = sdf.reset_index()\n",
    "\n",
    "\n",
    "for i in range(0,18):\n",
    "    print(\"\\n\", sdf.text_clean.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>5.2. Extract pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN      COARSE   FINE   DESCRIPTION FINE                                   DEPENDENCY DESCRIPTION DEPENDENCY\n",
      "-----      ------   ----   ----------------                                   ---------- ----------------------\n",
      "'          PUNCT    ''     closing quotation mark                             punct    punctuation\n",
      "shiok      NOUN     NN     noun, singular or mass                             nsubj    nominal subject\n",
      ",          PUNCT    ,      punctuation mark, comma                            punct    punctuation\n",
      "if         SCONJ    IN     conjunction, subordinating or preposition          mark     marker\n",
      "you        PRON     PRP    pronoun, personal                                  nsubj    nominal subject\n",
      "are        VERB     VBP    verb, non-3rd person singular present              advcl    adverbial clause modifier\n",
      "n't        PART     RB     adverb                                             neg      negation modifier\n",
      "familiar   ADJ      JJ     adjective (English), other noun-modifier (Chinese) acomp    adjectival complement\n",
      "with       ADP      IN     conjunction, subordinating or preposition          prep     prepositional modifier\n",
      "this       DET      DT     determiner                                         det      determiner\n",
      "part       NOUN     NN     noun, singular or mass                             pobj     object of preposition\n",
      "of         ADP      IN     conjunction, subordinating or preposition          prep     prepositional modifier\n",
      "the        DET      DT     determiner                                         det      determiner\n",
      "equator    NOUN     NN     noun, singular or mass                             pobj     object of preposition\n",
      ",          PUNCT    ,      punctuation mark, comma                            punct    punctuation\n",
      "is         AUX      VBZ    verb, 3rd person singular present                  ROOT     None\n",
      "a          DET      DT     determiner                                         det      determiner\n",
      "local      ADJ      JJ     adjective (English), other noun-modifier (Chinese) amod     adjectival modifier\n",
      "slang      NOUN     NN     noun, singular or mass                             attr     attribute\n",
      "in         ADP      IN     conjunction, subordinating or preposition          prep     prepositional modifier\n",
      "singapore  PROPN    NNP    noun, proper singular                              pobj     object of preposition\n",
      "that       DET      WDT    wh-determiner                                      nsubj    nominal subject\n",
      "describes  VERB     VBZ    verb, 3rd person singular present                  relcl    relative clause modifier\n",
      "all        DET      DT     determiner                                         det      determiner\n",
      "feelings   NOUN     NNS    noun, plural                                       dobj     direct object\n",
      "that       DET      WDT    wh-determiner                                      nsubj    nominal subject\n",
      "'s         VERB     VBZ    verb, 3rd person singular present                  relcl    relative clause modifier\n",
      "fantastic  ADJ      JJ     adjective (English), other noun-modifier (Chinese) acomp    adjectival complement\n",
      ",          PUNCT    ,      punctuation mark, comma                            punct    punctuation\n",
      "indescribable ADJ      JJ     adjective (English), other noun-modifier (Chinese) conj     conjunct\n",
      "or         CCONJ    CC     conjunction, coordinating                          cc       coordinating conjunction\n",
      "sensational ADJ      JJ     adjective (English), other noun-modifier (Chinese) conj     conjunct\n",
      "'          PUNCT    ''     closing quotation mark                             punct    punctuation\n"
     ]
    }
   ],
   "source": [
    "# Make your Doc object and pass it into the scorer:\n",
    "doc = nlp(sdf.text_clean.iloc[10])\n",
    "\n",
    "# For practice, visualize your fine-grained POS tags (shown in the third column):\n",
    "print(f\"{'TOKEN':{10}} {'COARSE':{8}} {'FINE':{6}} {'DESCRIPTION FINE':{50}} {'DEPENDENCY':{6}} {'DESCRIPTION DEPENDENCY'}\")\n",
    "print(f\"{'-----':{10}} {'------':{8}} {'----':{6}} {'----------------':{50}} {'----------':{6}} {'----------------------'}\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_):{50}} {token.dep_:{8}} {spacy.explain(token.dep_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the catch 22 of the singapore food scene is that there is no culture of institutional continuity in this lucrative and well-loved food industry.'\n"
     ]
    }
   ],
   "source": [
    "# extract pattern: N + adj\n",
    "\n",
    "def extract_pattern(doc):\n",
    "    \n",
    "    obj_list = []\n",
    "    adj_list = []\n",
    "    connection_list = []\n",
    "    conj_list = []\n",
    "    \n",
    "    flag = 0\n",
    "    for i, tok in enumerate(doc):\n",
    "        \n",
    "        # extract subject or direct object, it used to link other objects, noun\n",
    "        if((tok.pos_.endswith(\"NOUN\")==True) | (tok.pos_.endswith(\"PROPN\") == True)):\n",
    "            if((tok.dep_.endswith(\"dobj\")==True) | (tok.dep_.endswith(\"ROOT\")==True)| (tok.dep_.endswith(\"nsubj\")==True)):\n",
    "                #print(tok.text,tok.dep_)\n",
    "                connection_list.append(tok.text)  \n",
    "                #print(connection_list)\n",
    "            else:\n",
    "                connection_list.append('N/A')\n",
    "        else: \n",
    "            connection_list.append('N/A')\n",
    "        \n",
    "        # extract all nouns\n",
    "        if((tok.pos_.endswith(\"NOUN\")==True) | (tok.pos_.endswith(\"PROPN\") == True)):\n",
    "            if((tok.dep_.endswith(\"compound\")==False) & (tok.dep_.endswith(\"amod\")==False)):\n",
    "                obj_list.append(tok.text)\n",
    "            else:\n",
    "                obj_list.append('N/A')  \n",
    "        else:\n",
    "            obj_list.append('N/A')\n",
    "        \n",
    "        # extract compound => pattern: compound + noun (oxford street, convern garden)\n",
    "        if((tok.pos_.endswith(\"NOUN\")==True) | (tok.pos_.endswith(\"PROPN\") == True)):\n",
    "            if((tok.dep_.endswith(\"compound\")==True)):\n",
    "                conj_list.append(tok.text)\n",
    "            else:\n",
    "                conj_list.append('N/A')  \n",
    "        else:\n",
    "            conj_list.append('N/A')\n",
    "        \n",
    "        # extract adj\n",
    "        if((tok.pos_.endswith(\"ADJ\")==True)|(tok.dep_.endswith(\"amod\")==True)):\n",
    "            adj_list.append(tok.text)\n",
    "        else:\n",
    "            adj_list.append('N/A')\n",
    "            \n",
    "    return obj_list, adj_list, connection_list, conj_list\n",
    "\n",
    "doc = nlp(sdf.text_clean.iloc[15])\n",
    "print(sdf.text_clean.iloc[15])\n",
    "obj_list, adj_list, connection_list, conj_list = extract_pattern(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_obj1 = []\n",
    "list_obj2 = []\n",
    "connection_word = 'N/A'\n",
    "\n",
    "#for e in range(0, 1):\n",
    "for e in range(0, len(sdf)):\n",
    "    doc = nlp(sdf.text_clean.iloc[e])\n",
    "    \n",
    "    obj_list, adj_list, connection_list, conj_list = extract_pattern(doc)\n",
    "    \n",
    "    for i in range(0, len(obj_list)):\n",
    "        \n",
    "        # connection word {root noun, direct object + object}\n",
    "        if((connection_list[i] != 'N/A')):\n",
    "            #print(connection_list[i], obj_list[i])\n",
    "            connection_word = connection_list[i]\n",
    "       \n",
    "        #  nound + adj near by\n",
    "        if (obj_list[i] != 'N/A'):\n",
    "            # compount object + object\n",
    "            if(conj_list[i-1] != 'N/A'):\n",
    "                #list_obj1.append(obj_list[i])\n",
    "                #list_obj2.append(conj_list[i-1])  \n",
    "                obj_list[i] = conj_list[i-1] + ' ' + obj_list[i]\n",
    "                \n",
    "            # subject + noun\n",
    "            if((connection_word != obj_list[i]) & (connection_word != 'N/A')):\n",
    "                list_obj1.append(obj_list[i])\n",
    "                list_obj2.append(connection_word) \n",
    "            \n",
    "            # noun + adj on the left\n",
    "            for j in range(i-3, i):\n",
    "                if((j < i) & (j > i - 3) & (j >= 0 )):\n",
    "                    if (adj_list[j] != 'N/A'):\n",
    "                        #print(i, j)\n",
    "                        #print(obj_list[i], adj_list[j])\n",
    "                        list_obj1.append(obj_list[i])\n",
    "                        list_obj2.append(adj_list[j])\n",
    "            # noun + adj on the right   \n",
    "            for j in range(i+1, i+2):\n",
    "                if((j > i) & (j < i + 2) & (j < len(obj_list))):\n",
    "                    if (adj_list[j] != 'N/A'):\n",
    "                        #print(i, j)\n",
    "                        #print(obj_list[i], adj_list[j])\n",
    "                        list_obj1.append(obj_list[i])\n",
    "                        list_obj2.append(adj_list[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 151)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_obj1), len(list_obj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div class=\"alert alert-info\"> 6. Knowledge Graph </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>6.1. create data frame to contain nodes of graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj1</th>\n",
       "      <th>obj2</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate relationship</td>\n",
       "      <td>relationship</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>singapore</td>\n",
       "      <td>relationship</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>years</td>\n",
       "      <td>relationship</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>singapore airlines</td>\n",
       "      <td>relationship</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>class product</td>\n",
       "      <td>product</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 obj1          obj2  ID\n",
       "0   hate relationship  relationship   0\n",
       "1           singapore  relationship   1\n",
       "2               years  relationship   2\n",
       "3  singapore airlines  relationship   3\n",
       "4       class product       product   4"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_df = pd.DataFrame()\n",
    "graph_df['obj1'] = list_obj1\n",
    "graph_df['obj2'] = list_obj2\n",
    "graph_df['ID'] = range(0,len(graph_df))\n",
    "#graph_df = graph_df[graph_df.obj2 != 'NA']\n",
    "print(graph_df.shape)\n",
    "graph_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj1</th>\n",
       "      <th>obj2</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>year</td>\n",
       "      <td>last</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>people</td>\n",
       "      <td>most</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aesthetics</td>\n",
       "      <td>bridge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>singapore</td>\n",
       "      <td>city</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>services</td>\n",
       "      <td>personal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>helix design</td>\n",
       "      <td>bridge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>hilton</td>\n",
       "      <td>marketing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>hotel</td>\n",
       "      <td>iconic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>hotel</td>\n",
       "      <td>people</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>york city</td>\n",
       "      <td>city</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             obj1       obj2  weight\n",
       "145          year       last       2\n",
       "86         people       most       2\n",
       "0      aesthetics     bridge       1\n",
       "102     singapore       city       1\n",
       "96       services   personal       1\n",
       "..            ...        ...     ...\n",
       "50   helix design     bridge       1\n",
       "51         hilton  marketing       1\n",
       "52          hotel     iconic       1\n",
       "53          hotel     people       1\n",
       "148     york city       city       1\n",
       "\n",
       "[149 rows x 3 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_graph_df = pd.DataFrame(graph_df.groupby(['obj1','obj2']).count()).reset_index()\n",
    "weight_graph_df.rename(columns={'ID':'weight'}, inplace=True)\n",
    "weight_graph_df = weight_graph_df.drop_duplicates(subset=['obj1','obj2'], keep='last')\n",
    "weight_graph_df.sort_values(['weight'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>6.2. Network Analysis (with NetworkX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Name: Hotel Interactions Network\n",
      "Type: Graph\n",
      "Number of nodes: 154\n",
      "Number of edges: 149\n",
      "Average degree:   1.9351\n",
      "Network density: 0.012647483235718529\n"
     ]
    }
   ],
   "source": [
    "# Generate a networkx graph\n",
    "G = nx.from_pandas_edgelist(weight_graph_df, 'obj1', 'obj2')\n",
    "\n",
    "# Give the graph a name\n",
    "G.name = 'Hotel Interactions Network'\n",
    "\n",
    "# Check whether graph is directed or undirected (False = undirected)\n",
    "print(G.is_directed())\n",
    "\n",
    "# Obtain general information of graph\n",
    "print(nx.info(G))\n",
    "\n",
    "# Get graph density\n",
    "density = nx.density(G)\n",
    "print(\"Network density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('singapore', 28)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get most connected node (i.e. drug with most drug interactions)\n",
    "G.degree()\n",
    "max(dict(G.degree()).items(), key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>6.3. Network Visualization (with Pyvis)\n",
    "\n",
    "https://towardsdatascience.com/customizing-networkx-graphs-f80b4e69bedf\n",
    "\n",
    "https://www.cl.cam.ac.uk/teaching/1314/L109/tutorial.pdf\n",
    "\n",
    "https://www.toptal.com/data-science/graph-data-science-python-networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate Pyvis visualization\n",
    "def generate_network_viz(df, source_col, target_col, weights, \n",
    "                         layout='barnes_hut',\n",
    "                         central_gravity=0.15,\n",
    "                         node_distance=420,\n",
    "                         spring_length=100,\n",
    "                         spring_strength=0.15,\n",
    "                         damping=0.96,\n",
    "                         minium_weight: int = 0,\n",
    "                         ):\n",
    "    \n",
    "    # Generate a networkx graph\n",
    "    G = nx.from_pandas_edgelist(df, source_col, target_col, weights)\n",
    "    \n",
    "    if layout == 'repulsion':\n",
    "        bgcolor, font_color = '#222222', 'white'\n",
    "    else:\n",
    "        bgcolor, font_color = 'white', 'black'\n",
    "    \n",
    "    # Initiate PyVis network object\n",
    "    drug_net = Network(\n",
    "                       height='700px', \n",
    "                       width='100%',\n",
    "                       bgcolor=bgcolor, \n",
    "                       font_color=font_color, \n",
    "                       notebook=True\n",
    "                      )\n",
    "    \n",
    "    # Take Networkx graph and translate it to a PyVis graph format\n",
    "    drug_net.from_nx(G)\n",
    "    \n",
    "    # Create different network layout (repulsion or Barnes Hut)\n",
    "    if layout == 'repulsion':\n",
    "        drug_net.repulsion(\n",
    "                            node_distance=node_distance, \n",
    "                            central_gravity=central_gravity, \n",
    "                            spring_length=spring_length, \n",
    "                            spring_strength=spring_strength, \n",
    "                            damping=damping\n",
    "                           )\n",
    "        \n",
    "    # Run default Barnes Hut visualization\n",
    "    else:\n",
    "        drug_net.barnes_hut(\n",
    "#                            gravity=-80000, \n",
    "#                            central_gravity=central_gravity, \n",
    "#                            spring_length=spring_length, \n",
    "#                            spring_strength=spring_strength, \n",
    "#                            damping=damping, \n",
    "#                            overlap=0\n",
    "                          )      \n",
    "    return drug_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Barnes Hut Visualization\n",
    "BarnesHut is a quadtree based gravity model\n",
    "It is the fastest, default and recommended solver for non-hierarchical layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridge 9\n",
      "marketing 7\n",
      "singapore 28\n",
      "people 4\n",
      "feelings 3\n",
      "lattes 4\n",
      "cities 3\n",
      "city 4\n",
      "sling 9\n",
      "catch 5\n",
      "views 5\n",
      "travels 8\n",
      "notables 4\n",
      "terminal 4\n",
      "shiok 4\n",
      "fare 6\n",
      "centre 5\n",
      "need 3\n",
      "relationship 4\n",
      "hawker 3\n",
      "hotel 3\n",
      "hotels 3\n",
      "region 4\n",
      "services 3\n",
      "singapore airlines 3\n",
      "tech 3\n",
      "year 3\n"
     ]
    }
   ],
   "source": [
    "selected_nodes = []\n",
    "for e in dict(G.degree()).items():\n",
    "    node, degree = e\n",
    "    if ((degree < 100) & (degree > 2)):\n",
    "    # if (node == 'bridge'):\n",
    "        print(node, degree)\n",
    "        selected_nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"700px\"\n",
       "            src=\"hotel_interactions_network_room.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x19264d62670>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selected_nodes = ['location','italian', 'prime']\n",
    "# Create network for single drug. Use Phenytoin since it has most edges (i.e. involved in most drug interactions)\n",
    "small_nw = weight_graph_df.loc[weight_graph_df['obj1'].isin(selected_nodes) | weight_graph_df['obj2'].isin(selected_nodes)]\n",
    "\n",
    "node_color = {'NOUN':'lightblue', 'PRON':'lightblue', 'PROPN':'yellow', 'VERB': 'red', 'ADJ':'red', 'ADV':'red', 'DET':'red', 'X':'grey', 'INTJ':'grey',\n",
    "              'AUX':'grey','NUM':'grey','SPACE':'grey','PUNCT':'grey','SCONJ':'grey','ADP':'grey'}\n",
    "# Generate a networkx graph based on subset data\n",
    "net_repulsion = generate_network_viz(small_nw, 'obj1','obj2', 'weight', layout='repulsion')\n",
    "\n",
    "node_list = []\n",
    "for i in range(0, len(net_repulsion.nodes)):\n",
    "    node_list.append(net_repulsion.nodes[i]['id'])\n",
    "type_node = [nlp(x)[0].pos_ for x in node_list ]\n",
    "\n",
    "for i in range(0, len(net_repulsion.nodes)):\n",
    "    net_repulsion.nodes[i]['color'] = node_color[type_node[i]]\n",
    "\n",
    "net_repulsion.show('hotel_interactions_network_room.html')\n",
    "\n",
    "# Run the above code chunk in order to display the graph visualization below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(node_list)\n",
    "print(type_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

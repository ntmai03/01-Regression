{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-info\"> 2. Setup </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install networkx\n",
    "#!pip install datapane\n",
    "#!pip install operator\n",
    "#!pip install pyvis\n",
    "#!pip install streamlit pyvis networkx\n",
    "#!pip install langdetect\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import zipfile\n",
    "import json\n",
    "import urllib\n",
    "import langdetect\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy \n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community #This part of networkx, for community detection, needs to be imported separately\n",
    "import datapane as dp\n",
    "#from operator import itemgetter\n",
    "from pyvis.network import Network\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div class=\"alert alert-info\"> 3. Data Preparation </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-YAVgrS6CUM"
   },
   "source": [
    " ## <font color=red>**News category dataset**\n",
    "\n",
    "The dataset used is “News category dataset” from Kaggle (https://www.kaggle.com/rmisra/news-category-dataset. This dataset provided with around 200k news headlines from the year 2012 to 2018 obtained from HuffPost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Load data\n",
    "    \n",
    "The dataset is contained into a json file, so I will first read it into a list of dictionaries with json and then transform it into a pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26768, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>U.S. Launches Auto Import Probe, China Vows To...</td>\n",
       "      <td>The investigation could lead to new U.S. tarif...</td>\n",
       "      <td>U.S. Launches Auto Import Probe, China Vows To...</td>\n",
       "      <td>en</td>\n",
       "      <td>u launch auto import probe china vow defend in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Starbucks Says Anyone Can Now Sit In Its Cafes...</td>\n",
       "      <td>The new policy was unveiled weeks after the co...</td>\n",
       "      <td>Starbucks Says Anyone Can Now Sit In Its Cafes...</td>\n",
       "      <td>en</td>\n",
       "      <td>starbuck say anyon sit cafe even without buy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Seattle Passes Controversial New Tax On City's...</td>\n",
       "      <td>Following the council vote, Amazon’s vice pres...</td>\n",
       "      <td>Seattle Passes Controversial New Tax On City's...</td>\n",
       "      <td>en</td>\n",
       "      <td>seattl pas controversi new tax citi biggest co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Uber Ends Forced Arbitration In Individual Cas...</td>\n",
       "      <td>Victims will be free to go to court -- but a f...</td>\n",
       "      <td>Uber Ends Forced Arbitration In Individual Cas...</td>\n",
       "      <td>en</td>\n",
       "      <td>uber end forc arbitr individu case sexual assa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Chili's Hit By Data Breach, Credit And Debit C...</td>\n",
       "      <td>The breach is believed to have occurred betwee...</td>\n",
       "      <td>Chili's Hit By Data Breach, Credit And Debit C...</td>\n",
       "      <td>en</td>\n",
       "      <td>chili hit data breach credit debit card inform...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           headline  \\\n",
       "0  BUSINESS  U.S. Launches Auto Import Probe, China Vows To...   \n",
       "1  BUSINESS  Starbucks Says Anyone Can Now Sit In Its Cafes...   \n",
       "2  BUSINESS  Seattle Passes Controversial New Tax On City's...   \n",
       "3  BUSINESS  Uber Ends Forced Arbitration In Individual Cas...   \n",
       "4  BUSINESS  Chili's Hit By Data Breach, Credit And Debit C...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  The investigation could lead to new U.S. tarif...   \n",
       "1  The new policy was unveiled weeks after the co...   \n",
       "2  Following the council vote, Amazon’s vice pres...   \n",
       "3  Victims will be free to go to court -- but a f...   \n",
       "4  The breach is believed to have occurred betwee...   \n",
       "\n",
       "                                                text lang  \\\n",
       "0  U.S. Launches Auto Import Probe, China Vows To...   en   \n",
       "1  Starbucks Says Anyone Can Now Sit In Its Cafes...   en   \n",
       "2  Seattle Passes Controversial New Tax On City's...   en   \n",
       "3  Uber Ends Forced Arbitration In Individual Cas...   en   \n",
       "4  Chili's Hit By Data Breach, Credit And Debit C...   en   \n",
       "\n",
       "                                          text_clean  \n",
       "0  u launch auto import probe china vow defend in...  \n",
       "1  starbuck say anyon sit cafe even without buy a...  \n",
       "2  seattl pas controversi new tax citi biggest co...  \n",
       "3  uber end forc arbitr individu case sexual assa...  \n",
       "4  chili hit data breach credit debit card inform...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/News_Category.csv')\n",
    "print(df.shape)\n",
    "## print 5 random rows\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KIR46g5Y2uQs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRAVEL          9855\n",
       "FOOD & DRINK    6208\n",
       "BUSINESS        5878\n",
       "SPORTS          4827\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocess a string.\n",
    ":parameter\n",
    "    :param text: string - name of column containing text\n",
    "    :param lst_stopwords: list - list of stopwords to remove\n",
    "    :param flg_stemm: bool - whether stemming is to be applied\n",
    "    :param flg_lemm: bool - whether lemmitisation is to be applied\n",
    ":return\n",
    "    cleaned text\n",
    "'''\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=False, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and   characters and then strip)\n",
    "    #text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    #text = re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    text = str(text).lower().strip()\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>U.S. Launches Auto Import Probe, China Vows To...</td>\n",
       "      <td>The investigation could lead to new U.S. tarif...</td>\n",
       "      <td>U.S. Launches Auto Import Probe, China Vows To...</td>\n",
       "      <td>en</td>\n",
       "      <td>the investigation could lead to new u.s. tarif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Starbucks Says Anyone Can Now Sit In Its Cafes...</td>\n",
       "      <td>The new policy was unveiled weeks after the co...</td>\n",
       "      <td>Starbucks Says Anyone Can Now Sit In Its Cafes...</td>\n",
       "      <td>en</td>\n",
       "      <td>the new policy was unveiled weeks after the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Seattle Passes Controversial New Tax On City's...</td>\n",
       "      <td>Following the council vote, Amazon’s vice pres...</td>\n",
       "      <td>Seattle Passes Controversial New Tax On City's...</td>\n",
       "      <td>en</td>\n",
       "      <td>following the council vote, amazon’s vice pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Uber Ends Forced Arbitration In Individual Cas...</td>\n",
       "      <td>Victims will be free to go to court -- but a f...</td>\n",
       "      <td>Uber Ends Forced Arbitration In Individual Cas...</td>\n",
       "      <td>en</td>\n",
       "      <td>victims will be free to go to court -- but a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>Chili's Hit By Data Breach, Credit And Debit C...</td>\n",
       "      <td>The breach is believed to have occurred betwee...</td>\n",
       "      <td>Chili's Hit By Data Breach, Credit And Debit C...</td>\n",
       "      <td>en</td>\n",
       "      <td>the breach is believed to have occurred betwee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                           headline  \\\n",
       "0  BUSINESS  U.S. Launches Auto Import Probe, China Vows To...   \n",
       "1  BUSINESS  Starbucks Says Anyone Can Now Sit In Its Cafes...   \n",
       "2  BUSINESS  Seattle Passes Controversial New Tax On City's...   \n",
       "3  BUSINESS  Uber Ends Forced Arbitration In Individual Cas...   \n",
       "4  BUSINESS  Chili's Hit By Data Breach, Credit And Debit C...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  The investigation could lead to new U.S. tarif...   \n",
       "1  The new policy was unveiled weeks after the co...   \n",
       "2  Following the council vote, Amazon’s vice pres...   \n",
       "3  Victims will be free to go to court -- but a f...   \n",
       "4  The breach is believed to have occurred betwee...   \n",
       "\n",
       "                                                text lang  \\\n",
       "0  U.S. Launches Auto Import Probe, China Vows To...   en   \n",
       "1  Starbucks Says Anyone Can Now Sit In Its Cafes...   en   \n",
       "2  Seattle Passes Controversial New Tax On City's...   en   \n",
       "3  Uber Ends Forced Arbitration In Individual Cas...   en   \n",
       "4  Chili's Hit By Data Breach, Credit And Debit C...   en   \n",
       "\n",
       "                                          text_clean  \n",
       "0  the investigation could lead to new u.s. tarif...  \n",
       "1  the new policy was unveiled weeks after the co...  \n",
       "2  following the council vote, amazon’s vice pres...  \n",
       "3  victims will be free to go to court -- but a f...  \n",
       "4  the breach is believed to have occurred betwee...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "df['text_clean'] = df[\"short_description\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=False, lst_stopwords=None))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['category'] == 'TRAVEL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Split text to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16747, 1)\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "sent_list = []\n",
    "id_list = []\n",
    "category_list = []\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    doc = nlp(df.text_clean.iloc[i])\n",
    "    for sent in doc.sents:\n",
    "        id_list.append(i)\n",
    "        sent = \"'\" + str(sent) + \"'\"\n",
    "        sent_list.append(sent)\n",
    "        category_list.append(df.category.iloc[i])\n",
    "        \n",
    "sent_df = pd.DataFrame()\n",
    "#sent_df['id'] = id_list\n",
    "sent_df['text_clean'] = sent_list\n",
    "print(sent_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16747, 1)\n"
     ]
    }
   ],
   "source": [
    "#sent_df = pd.read_csv('Category_News_sent_df.csv')\n",
    "sent_df = pd.DataFrame()\n",
    "#sent_df['id'] = id_list\n",
    "sent_df['text_clean'] = sent_list\n",
    "print(sent_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'having waterproof covers on the seats is kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'charming towns, relaxing beaches and top hiki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'star wars: galaxy's edge will open at disneyl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'these underrated travel destinations in europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'if you’re dreaming about a romantic european ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean\n",
       "0  'having waterproof covers on the seats is kind...\n",
       "1  'charming towns, relaxing beaches and top hiki...\n",
       "2  'star wars: galaxy's edge will open at disneyl...\n",
       "3  'these underrated travel destinations in europ...\n",
       "4  'if you’re dreaming about a romantic european ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <div class=\"alert alert-info\"> 4. Information Retrieval </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find documents contains words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156, 1)\n",
      "\n",
      " 'if only we could apparate ourselves to london.'\n",
      "\n",
      " 'whenever jackie and i are planning a trip to london (which is as often as possible), we check to see what’s playing at the'\n",
      "\n",
      " 'doug's trip to london was paws-itively precious.'\n",
      "\n",
      " 'eye see you the baku eye is azerbaijan’s version of the london eye.'\n",
      "\n",
      " 'london is a tourist’s paradise owing to the several iconic attractions it has to offer.therefore,it makes sense why london'\n",
      "\n",
      " 'london is giving new meaning to slip and slide.'\n"
     ]
    }
   ],
   "source": [
    "sent_df.text_clean = sent_df.text_clean.astype(str) \n",
    "\n",
    "\n",
    "sdf = sent_df[sent_df['text_clean'].str.contains(\"london\")]\n",
    "print(sdf.shape)\n",
    "sdf = sdf.reset_index()\n",
    "\n",
    "for i in range(0,6):\n",
    "    print(\"\\n\", sdf.text_clean.iloc[i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type 1: adj + N\n",
    "type 2: N + adj\n",
    "type 3: compound + N\n",
    "type 4:  N + N\n",
    "    + 'singapore is currently the most competitive city in the world, beating out new york and london, according to the economist'\n",
    "    + 'challengers to silicon valley include new york, l.a., boston, tel aviv, and london.'\n",
    "    + ed young is the senior pastor of fellowship church, which is headquartered in grapevine, texas but has rapidly expanded across texas, to florida, london (uk) and online.\n",
    "type 5:  S + V\n",
    "    + 'she earned her bfa at london college of communication, uk'\n",
    "    + 'prices soared as people scrambled to flee the london bridge attack'\n",
    "type 6: N + relative clause\n",
    "    + ed young is the senior pastor of fellowship church, which is headquartered in grapevine, texas but has rapidly expanded across texas, to florida, london (uk) and online.\n",
    "type 7: complecated\n",
    "    + 'london -- a classic mercedes-benz race car driven by formula 1 legend juan manuel fangio sold for 19.6 million pounds ($29.6'\n",
    "    + \"at a london branch of britain's biggest retailer, tesco , which found horse dna in some of its own-brand frozen spaghetti\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN      COARSE   FINE   DESCRIPTION FINE                                   DEPENDENCY DESCRIPTION DEPENDENCY\n",
      "-----      ------   ----   ----------------                                   ---------- ----------------------\n",
      "according  VERB     VBG    verb, gerund or present participle                 prep     prepositional modifier\n",
      "to         ADP      IN     conjunction, subordinating or preposition          prep     prepositional modifier\n",
      "a          DET      DT     determiner                                         det      determiner\n",
      "newly      ADV      RB     adverb                                             advmod   adverbial modifier\n",
      "released   VERB     VBN    verb, past participle                              amod     adjectival modifier\n",
      "report     NOUN     NN     noun, singular or mass                             pobj     object of preposition\n",
      ",          PUNCT    ,      punctuation mark, comma                            punct    punctuation\n",
      "the        DET      DT     determiner                                         det      determiner\n",
      "united     PROPN    NNP    noun, proper singular                              compound compound\n",
      "states     PROPN    NNP    noun, proper singular                              nsubjpass nominal subject (passive)\n",
      "is         AUX      VBZ    verb, 3rd person singular present                  auxpass  auxiliary (passive)\n",
      "predicted  VERB     VBN    verb, past participle                              ROOT     None\n",
      "to         PART     TO     infinitival \"to\"                                   aux      auxiliary\n",
      "win        VERB     VB     verb, base form                                    xcomp    open clausal complement\n",
      "the        DET      DT     determiner                                         det      determiner\n",
      "most       ADJ      JJS    adjective, superlative                             amod     adjectival modifier\n",
      "medals     NOUN     NNS    noun, plural                                       dobj     direct object\n",
      "at         ADP      IN     conjunction, subordinating or preposition          prep     prepositional modifier\n",
      "the        DET      DT     determiner                                         det      determiner\n",
      "london     PROPN    NNP    noun, proper singular                              nmod     modifier of nominal\n",
      "2012       NUM      CD     cardinal number                                    nummod   numeric modifier\n",
      "olympics   NOUN     NNS    noun, plural                                       pobj     object of preposition\n",
      ".          PUNCT    .      punctuation mark, sentence closer                  punct    punctuation\n"
     ]
    }
   ],
   "source": [
    "sentence = 'singapore is currently the most competitive city in the world, beating out new york and london, according to the economist'\n",
    "sentence = 'challengers to silicon valley include new york, l.a., boston, tel aviv, and london.'\n",
    "sentence = 'ed young is the senior pastor of fellowship church, which is headquartered in grapevine, texas but has rapidly expanded across texas, to florida, london (uk) and online.'\n",
    "sentence = 'she earned her bfa at london college of communication, uk'\n",
    "sentence = 'london -- a classic mercedes-benz race car driven by formula 1 legend juan manuel fangio sold for 19.6 million pounds ($29.6'\n",
    "sentence  = 'prices soared as people scrambled to flee the london bridge attack'\n",
    "sentence= \"at a london branch of britain's biggest retailer, tesco , which found horse dna in some of its own-brand frozen spaghetti\"\n",
    "sentence = 'maria perez is the co-founder and product manager of glassful'\n",
    "sentence = 'london is a tourist’s paradise owing to the several iconic attractions it has to offer.therefore,it makes sense why london'\n",
    "sentence = \"investigators in washington and london last month struck a $450 million settlement with barclays in a rate-rigging case, but\"\n",
    "sentence = 'according to a newly released report, the united states is predicted to win the most medals at the london 2012 olympics.'\n",
    "\n",
    "# Make your Doc object and pass it into the scorer:\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# For practice, visualize your fine-grained POS tags (shown in the third column):\n",
    "print(f\"{'TOKEN':{10}} {'COARSE':{8}} {'FINE':{6}} {'DESCRIPTION FINE':{50}} {'DEPENDENCY':{6}} {'DESCRIPTION DEPENDENCY'}\")\n",
    "print(f\"{'-----':{10}} {'------':{8}} {'----':{6}} {'----------------':{50}} {'----------':{6}} {'----------------------'}\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_):{50}} {token.dep_:{8}} {spacy.explain(token.dep_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['report', 'medals']\n",
      "['released', 'most']\n"
     ]
    }
   ],
   "source": [
    "# extract pattern: adj + N\n",
    "def extract_pattern1(doc):\n",
    "    \n",
    "    adj = \"\"\n",
    "    obj1 = []\n",
    "    obj2 = []\n",
    "    for i, tok in enumerate(doc):\n",
    "        #print(i, \": \", tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)\n",
    "        \n",
    "        # structure adj + Noun: capture adj or gerund or Participant as adj ()\n",
    "        # if((tok.dep_.endswith(\"acomp\") == False) & (tok.pos_.endswith(\"ADJ\") == True)):\n",
    "        if(tok.pos_.endswith(\"ADJ\") == True):\n",
    "            adj = tok.text\n",
    "        elif((tok.dep_.endswith(\"amod\") == True) & (tok.pos_.endswith(\"VERB\") == True)):\n",
    "            adj = tok.text\n",
    "            \n",
    "            \n",
    "        if((tok.pos_.endswith(\"NOUN\")==True) | (tok.pos_.endswith(\"PROPN\") == True)):\n",
    "            if(tok.dep_.endswith(\"compound\") == False):\n",
    "                entity = tok.text\n",
    "                if(len(adj) > 0):\n",
    "                    obj1.append(entity)\n",
    "                    obj2.append(adj)\n",
    "                adj = \"\"\n",
    "\n",
    "    \n",
    "    return obj1, obj2\n",
    "            \n",
    "clause = 'location was great, just a few minute walk to supermarket'\n",
    "obj1, obj2 = extract_pattern1(nlp(sentence))\n",
    "print(obj1)\n",
    "print(obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['states']\n",
      "['most']\n"
     ]
    }
   ],
   "source": [
    "# extract pattern: N + adj\n",
    "\n",
    "def extract_pattern2(doc):\n",
    "    \n",
    "    entity = \"\"\n",
    "    obj1 = []\n",
    "    obj2 = []\n",
    "    flag = 0\n",
    "    for i, tok in enumerate(doc):\n",
    "        # print(i, \": \", tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)\n",
    "            \n",
    "        # extract subject/ root\n",
    "        if((tok.pos_.endswith(\"NOUN\")==True) | (tok.pos_.endswith(\"PROPN\") == True)):\n",
    "            if((tok.dep_.endswith(\"ROOT\") == True) | (tok.dep_.endswith(\"appos\") == True) | (tok.dep_.find(\"subj\") == True)):\n",
    "                entity = tok.text\n",
    "            else:\n",
    "                entity = \"\"\n",
    "                \n",
    "        if((tok.pos_.endswith(\"AUX\")==True)):\n",
    "            flag = 1\n",
    "                \n",
    "        # structure adj + Noun: capture adj or gerund or Participant as adj ()\n",
    "        # if((tok.dep_.endswith(\"acomp\") == False) & (tok.pos_.endswith(\"ADJ\") == True)):\n",
    "        if(tok.pos_.endswith(\"ADJ\") == True):\n",
    "            adj = tok.text\n",
    "            if((len(entity) > 0) & (flag==1)):\n",
    "                obj1.append(entity)\n",
    "                obj2.append(adj)\n",
    "            entity = \"\"\n",
    "            \n",
    "    \n",
    "    return obj1, obj2\n",
    "\n",
    "clause = 'Great location for shops, restaurants and access to public transport to explore London'\n",
    "obj1, obj2 = extract_pattern2(nlp(sentence))\n",
    "print(obj1)\n",
    "print(obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['states', 'states', 'states']\n",
      "['report', 'medals', 'olympics']\n"
     ]
    }
   ],
   "source": [
    "# extract pattern: Sbj + N + N +..+N\n",
    "\n",
    "def extract_pattern3(doc):\n",
    "    \n",
    "    entity1 = ''\n",
    "    entity2 = ''\n",
    "    obj1 = []\n",
    "    obj2 = []\n",
    "    flag = 0\n",
    "    for i, tok in enumerate(doc):\n",
    "        # print(i, \": \", tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)\n",
    "            \n",
    "        \n",
    "        if(tok.dep_.endswith(\"compound\") == False):\n",
    "            # extract subject/ root\n",
    "            if((tok.pos_.endswith(\"NOUN\")==True) | (tok.pos_.endswith(\"PROPN\") == True) ):\n",
    "                #print(tok.text, tok.dep_)\n",
    "                #if((tok.dep_.endswith(\"ROOT\") == True) | (tok.dep_.endswith(\"appos\") == True) | (tok.dep_.find(\"subj\") == True)):\n",
    "                if((tok.dep_.endswith(\"ROOT\") == True) | (tok.dep_.endswith(\"subj\") == True) | (tok.dep_.endswith(\"nsubjpass\") == True)):\n",
    "                    entity1 = tok.text\n",
    "                elif((tok.dep_.endswith('poss') == True) | (tok.dep_.endswith('attr') == True) | (tok.dep_.endswith('pobj') == True) | (tok.dep_.endswith('dobj') == True)| (tok.dep_.endswith('conj') == True)):\n",
    "                        entity2 = tok.text\n",
    "        \n",
    "        if((len(entity1) > 0) & (len(entity2) > 0)):\n",
    "            #print(entity1, entity2)\n",
    "            obj1.append(entity1)\n",
    "            obj2.append(entity2)\n",
    "            entity2= ''\n",
    "        '''     \n",
    "        elif((len(entity1) == 0) & (len(entity2) > 0)):\n",
    "            obj1.append('N/A')\n",
    "            obj2.append(entity2)\n",
    "            entity2= ''\n",
    "        '''\n",
    "            \n",
    "            \n",
    "    \n",
    "    return obj1, obj2\n",
    "\n",
    "clause = 'location was great, just a few minute walk to supermarket'\n",
    "obj1, obj2 = extract_pattern3(nlp(sentence))\n",
    "print(obj1)\n",
    "print(obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['united']\n",
      "['states']\n"
     ]
    }
   ],
   "source": [
    "# extract compound \n",
    "\n",
    "def extract_pattern4(doc):\n",
    "    adj = \"\"\n",
    "    obj1 = []\n",
    "    obj2 = []\n",
    "    for i, tok in enumerate(doc):\n",
    "        if((tok.pos_.endswith(\"NOUN\")==True) | (tok.pos_.endswith(\"PROPN\") == True)):\n",
    "            if(tok.dep_.endswith(\"compound\") == True):\n",
    "                adj = tok.text\n",
    "            \n",
    "            \n",
    "        if((tok.pos_.endswith(\"NOUN\")==True) | (tok.pos_.endswith(\"PROPN\") == True)):\n",
    "            if(tok.dep_.endswith(\"compound\") == False):\n",
    "                entity = tok.text\n",
    "                if(len(adj) > 0):\n",
    "                    obj1.append(adj)\n",
    "                    obj2.append(entity)\n",
    "                adj = \"\"\n",
    "\n",
    "    \n",
    "    return obj1, obj2\n",
    "\n",
    "obj1, obj2 = extract_pattern4(nlp(sentence))\n",
    "print(obj1)\n",
    "print(obj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['report', 'medals', 'states', 'states', 'states', 'states', 'united'],\n",
       " ['released', 'most', 'most', 'report', 'medals', 'olympics', 'states'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_info(clause):\n",
    "    list_obj1 = []\n",
    "    list_obj2 = []\n",
    "    obj1, obj2 = extract_pattern1(nlp(clause))\n",
    "    list_obj1 = list_obj1 + obj1\n",
    "    list_obj2 = list_obj2 + obj2\n",
    "    obj1, obj2 = extract_pattern2(nlp(clause))\n",
    "    list_obj1 = list_obj1 + obj1\n",
    "    list_obj2 = list_obj2 + obj2\n",
    "    obj1, obj2 = extract_pattern3(nlp(clause))\n",
    "    list_obj1 = list_obj1 + obj1\n",
    "    list_obj2 = list_obj2 + obj2\n",
    "    obj1, obj2 = extract_pattern4(nlp(clause))\n",
    "    list_obj1 = list_obj1 + obj1\n",
    "    list_obj2 = list_obj2 + obj2\n",
    "    \n",
    "    return list_obj1, list_obj2\n",
    "\n",
    "clause = 'location was good for me , off street parking'\n",
    "list_obj1, list_obj2 = extract_info(sentence)\n",
    "list_obj1, list_obj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_obj1 = []\n",
    "list_obj2 = []\n",
    "\n",
    "#for i in range(0, len(cluster_df)):\n",
    "for i in range(0, len(sdf)):\n",
    "    sent = sdf.text_clean.iloc[i]\n",
    "    #print(sent)\n",
    "    #sent = utils_preprocess_text(sent)\n",
    "    node1, node2 = extract_info(sent)\n",
    "    #print(node1)\n",
    "    #print(node2)\n",
    "    list_obj1 = list_obj1 + node1\n",
    "    list_obj2 = list_obj2 + node2\n",
    "        \n",
    "#print(house_obj1)\n",
    "#print(house_obj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create data frame to contain nodes of graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(745, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj1</th>\n",
       "      <th>obj2</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jackie</td>\n",
       "      <td>trip</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jackie</td>\n",
       "      <td>london</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trip</td>\n",
       "      <td>doug</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trip</td>\n",
       "      <td>london</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trip</td>\n",
       "      <td>paws</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     obj1    obj2  ID\n",
       "0  jackie    trip   0\n",
       "1  jackie  london   1\n",
       "2    trip    doug   2\n",
       "3    trip  london   3\n",
       "4    trip    paws   4"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_df = pd.DataFrame()\n",
    "graph_df['obj1'] = list_obj1\n",
    "graph_df['obj2'] = list_obj2\n",
    "graph_df['ID'] = range(0,len(graph_df))\n",
    "#graph_df = graph_df[graph_df.obj2 != 'NA']\n",
    "print(graph_df.shape)\n",
    "graph_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj1</th>\n",
       "      <th>obj2</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>new</td>\n",
       "      <td>york</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>summer</td>\n",
       "      <td>olympics</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>london</td>\n",
       "      <td>world</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>london</td>\n",
       "      <td>olympics</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>london</td>\n",
       "      <td>city</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>fountain</td>\n",
       "      <td>london</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>fountain</td>\n",
       "      <td>rome</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>fountain</td>\n",
       "      <td>trevi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>francisco</td>\n",
       "      <td>brand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>york</td>\n",
       "      <td>warmer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          obj1      obj2  weight\n",
       "455        new      york       6\n",
       "613     summer  olympics       4\n",
       "410     london     world       4\n",
       "366     london  olympics       3\n",
       "331     london      city       3\n",
       "..         ...       ...     ...\n",
       "235   fountain    london       1\n",
       "236   fountain      rome       1\n",
       "237   fountain     trevi       1\n",
       "238  francisco     brand       1\n",
       "703       york    warmer       1\n",
       "\n",
       "[704 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_graph_df = pd.DataFrame(graph_df.groupby(['obj1','obj2']).count()).reset_index()\n",
    "weight_graph_df.rename(columns={'ID':'weight'}, inplace=True)\n",
    "weight_graph_df = weight_graph_df.drop_duplicates(subset=['obj1','obj2'], keep='last')\n",
    "weight_graph_df.sort_values(['weight'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis (with NetworkX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Name: Hotel Interactions Network\n",
      "Type: Graph\n",
      "Number of nodes: 626\n",
      "Number of edges: 697\n",
      "Average degree:   2.2268\n",
      "Network density: 0.0035629392971246007\n"
     ]
    }
   ],
   "source": [
    "# Generate a networkx graph\n",
    "G = nx.from_pandas_edgelist(weight_graph_df, 'obj1', 'obj2')\n",
    "\n",
    "# Give the graph a name\n",
    "G.name = 'Hotel Interactions Network'\n",
    "\n",
    "# Check whether graph is directed or undirected (False = undirected)\n",
    "print(G.is_directed())\n",
    "\n",
    "# Obtain general information of graph\n",
    "print(nx.info(G))\n",
    "\n",
    "# Get graph density\n",
    "density = nx.density(G)\n",
    "print(\"Network density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('london', 129)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get most connected node (i.e. drug with most drug interactions)\n",
    "G.degree()\n",
    "max(dict(G.degree()).items(), key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Visualization (with Pyvis)\n",
    "\n",
    "https://towardsdatascience.com/customizing-networkx-graphs-f80b4e69bedf\n",
    "\n",
    "https://www.cl.cam.ac.uk/teaching/1314/L109/tutorial.pdf\n",
    "\n",
    "\n",
    "https://www.toptal.com/data-science/graph-data-science-python-networkx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate Pyvis visualization\n",
    "def generate_network_viz(df, source_col, target_col, weights, \n",
    "                         layout='barnes_hut',\n",
    "                         central_gravity=0.15,\n",
    "                         node_distance=420,\n",
    "                         spring_length=100,\n",
    "                         spring_strength=0.15,\n",
    "                         damping=0.96,\n",
    "                         minium_weight: int = 0,\n",
    "                         ):\n",
    "    \n",
    "    # Generate a networkx graph\n",
    "    G = nx.from_pandas_edgelist(df, source_col, target_col, weights)\n",
    "    \n",
    "    if layout == 'repulsion':\n",
    "        bgcolor, font_color = '#222222', 'white'\n",
    "    else:\n",
    "        bgcolor, font_color = 'white', 'black'\n",
    "    \n",
    "    # Initiate PyVis network object\n",
    "    drug_net = Network(\n",
    "                       height='700px', \n",
    "                       width='100%',\n",
    "                       bgcolor=bgcolor, \n",
    "                       font_color=font_color, \n",
    "                       notebook=True\n",
    "                      )\n",
    "    \n",
    "    # Take Networkx graph and translate it to a PyVis graph format\n",
    "    drug_net.from_nx(G)\n",
    "    \n",
    "    # Create different network layout (repulsion or Barnes Hut)\n",
    "    if layout == 'repulsion':\n",
    "        drug_net.repulsion(\n",
    "                            node_distance=node_distance, \n",
    "                            central_gravity=central_gravity, \n",
    "                            spring_length=spring_length, \n",
    "                            spring_strength=spring_strength, \n",
    "                            damping=damping\n",
    "                           )\n",
    "        \n",
    "    # Run default Barnes Hut visualization\n",
    "    else:\n",
    "        drug_net.barnes_hut(\n",
    "#                            gravity=-80000, \n",
    "#                            central_gravity=central_gravity, \n",
    "#                            spring_length=spring_length, \n",
    "#                            spring_strength=spring_strength, \n",
    "#                            damping=damping, \n",
    "#                            overlap=0\n",
    "                          )      \n",
    "    return drug_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barnes Hut Visualization\n",
    "BarnesHut is a quadtree based gravity model\n",
    "It is the fastest, default and recommended solver for non-hierarchical layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouthfuls 6\n",
      "airport 7\n",
      "fan 10\n",
      "passengers 7\n",
      "royal 7\n",
      "boom 6\n",
      "hotels 7\n",
      "building 6\n",
      "capital 6\n",
      "chances 6\n",
      "york 11\n",
      "charles 6\n",
      "city 14\n",
      "paris 7\n",
      "time 6\n",
      "place 7\n",
      "craig 6\n",
      "day 11\n",
      "trip 10\n",
      "shopping 6\n",
      "world 9\n",
      "eros 7\n",
      "events 6\n",
      "olympics 8\n",
      "top 8\n",
      "eyes 12\n",
      "folks 7\n",
      "best 10\n",
      "hundreds 6\n",
      "knightsbridge 8\n",
      "library 6\n",
      "list 6\n",
      "pubs 6\n",
      "travel 6\n",
      "neighborhood 11\n",
      "plan 8\n",
      "overs 6\n",
      "stories 9\n",
      "summer 6\n",
      "way 10\n"
     ]
    }
   ],
   "source": [
    "selected_nodes = []\n",
    "for e in dict(G.degree()).items():\n",
    "    node, degree = e\n",
    "    if ((degree < 100) & (degree > 5)):\n",
    "    # if (node == 'bridge'):\n",
    "        print(node, degree)\n",
    "        selected_nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"700px\"\n",
       "            src=\"hotel_interactions_network_room.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x13c16beb760>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selected_nodes = ['location','italian', 'prime']\n",
    "# Create network for single drug. Use Phenytoin since it has most edges (i.e. involved in most drug interactions)\n",
    "small_nw = weight_graph_df.loc[weight_graph_df['obj1'].isin(selected_nodes) | weight_graph_df['obj2'].isin(selected_nodes)]\n",
    "\n",
    "node_color = {'NOUN':'lightblue', 'PRON':'lightblue', 'PROPN':'yellow', 'VERB': 'red', 'ADJ':'red', 'ADV':'red', 'DET':'red', 'X':'grey', 'INTJ':'grey',\n",
    "              'AUX':'grey','NUM':'grey','SPACE':'grey','PUNCT':'grey','SCONJ':'grey','ADP':'grey'}\n",
    "# Generate a networkx graph based on subset data\n",
    "net_repulsion = generate_network_viz(small_nw, 'obj1','obj2', 'weight', layout='repulsion')\n",
    "\n",
    "node_list = []\n",
    "for i in range(0, len(net_repulsion.nodes)):\n",
    "    node_list.append(net_repulsion.nodes[i]['id'])\n",
    "type_node = [nlp(x)[0].pos_ for x in node_list ]\n",
    "\n",
    "for i in range(0, len(net_repulsion.nodes)):\n",
    "    net_repulsion.nodes[i]['color'] = node_color[type_node[i]]\n",
    "\n",
    "net_repulsion.show('hotel_interactions_network_room.html')\n",
    "\n",
    "# Run the above code chunk in order to display the graph visualization below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(node_list)\n",
    "print(type_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
